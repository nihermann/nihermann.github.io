<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Nicolai Hermann</title>

    <meta name="author" content="Nicolai Hermann">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Nicolai Hermann
                </p>
                <p>I'm a research assistant at <a href="https://www.pdf.inf.usi.ch/index.html" target="_blank">The Perception, Display, and Fabrication Group</a> at IDSIA-USI, in Lugano (Switzerland), where I'm working on 3D scene reconstruction.
                </p>
<!--                <p>-->
<!--                  At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.-->
<!--                </p>-->
                <p style="text-align:center">
                  <a href="mailto:hermann.nicolai@gmail.com" target="_blank">Email</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/nicolai-hermann-9598121b5/" target="_blank">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="data/CV_Nicolai_Hermann.pdf" target="_blank">CV</a> &nbsp;/&nbsp;
<!--                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;-->
                  <a href="https://github.com/nihermann/" target="_blank">Github</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=de&user=8v0OfD8AAAAJ" target="_blank">Scholar</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/nicolai_hermann_profile.jpg"><img style="width:100%;max-width:100%;object-fit: contain; border-radius: 50%;" alt="profile photo" src="images/nicolai_hermann_profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
<!--                <p>-->
<!--                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.-->
<!--                </p>-->
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="puzzle_stop()" onmouseover="puzzle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='puzzle_image'>
                  <img src='images/puzzle_before.png' style="width: 175px; height: 175px; object-fit: cover; object-position: 50% 5%;">
                </div>
                <img src='images/puzzle_after.png' style="width: 175px; height: 175px; object-fit: cover; object-position: 50% 5%;">
              </div>
              <script type="text/javascript">
                function puzzle_start() {
                  document.getElementById('puzzle_image').style.opacity = "1";
                }

                function puzzle_stop() {
                  document.getElementById('puzzle_image').style.opacity = "0";
                }
                puzzle_stop()
              </script>
            </td>
            <td style="padding:20px 20px 0px 20px;width:75%;vertical-align:middle">
              <a href="puzzlesim/index.html">
                <span class="papertitle">Puzzle Similarity: A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions</span>
              </a>
              <br>
              <strong>Nicolai Hermann</strong>,
              <a href="https://arcanous98.github.io/">Jorge Condor</a>,
              <a href="https://www.pdf.inf.usi.ch/people/piotr/">Piotr Didyk</a>
              <br>
              <em>ICCV</em>, 2025
              <br>
              <a href="puzzlesim/index.html">Project Page</a>
              /
              <a href="https://arxiv.org/abs/2411.17489">arXiv</a>
              /
              <a href="https://github.com/nihermann/PuzzleSim">Code</a>
              /
              <a href="https://huggingface.co/datasets/nihermann/annotated-3DGS-artifacts">Data</a>
              /
              <a href="puzzlesim/data/Puzzle_Similarity_Supplemental.pdf">Supplemental</a>
              <p></p>
              <p>
                Automatic 2D quality map generation for novel views assessing the quality of 3D scene reconstructions beyond supervised areas.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px 20px 69px 20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/thesis_front.jpg' style="width: 175px; height: 175px; object-fit: cover; object-position: 50% 5%;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://thesis.bul.sbu.usi.ch/theses/2340-2324Hermann/pdf?1730797532">
                <span class="papertitle">Perceptually-Driven Neural Inpainting for Seamless 3D Reconstructions</span>
              </a>
              <br>
              Supervision: <a href="https://arcanous98.github.io/">Jorge Condor</a>,
              <a href="https://www.pdf.inf.usi.ch/people/piotr/">Piotr Didyk</a>
              <br>
              Awarded the "<em style="color: red">Best Thesis Award</em>" from the <a href="https://www.fondazionepremio.ch/">Premio Swiss Engineering Ticino</a> foundation.
              <br>
              <p></p>
              <p>
                In my master’s thesis, I improved novel Scene Reconstruction methods, such as Gaussian Splatting. I introduced a new approach that assesses reconstruction quality by leveraging the input multiview content as priors to evaluate novel views. I then demonstrated the method’s effectiveness by devising a strategy that hallucinates incorrectly reconstructed parts of a scene.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px 20px 45px 20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two">
                   <img src='images/ploygcl_profile_white.png' style="width: 175px; height: 175px; object-fit: cover; object-position: 50% 5%;">
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/Reproducibility_Challenge_PolyGCL.pdf">
                <span class="papertitle">Reproducibility Challenge—PolyGCL: GRAPH CONTRASTIVE LEARNING via Learnable Spectral Polynomial Filters</span>
              </a>
              <br>
              Nicolai Hermann,
              <a href="https://www.linkedin.com/in/michele-cattaneo-614527210/">Michele Cattaneo</a>,
              <a href="https://www.linkedin.com/in/oliver-tryding-954171266/">Oliver Tryding</a>
              <br>
              <a href="data/Reproducibility_Challenge_PolyGCL.pdf">Project Report</a>
              /
              <a href="https://github.com/OliverTryding/-RE-POLYGCL">Reproduced Code</a>
              /
              <a href="https://wandb.ai/nihermann/RE-PolyGCL">WandB</a>
              /
              <a href="https://openreview.net/pdf?id=y21ZO6M86t">Original Paper</a>
              /
              <a href="https://github.com/ChenJY-Count/PolyGCL">Original Code</a>
              <p></p>
              <p>
                In this paper, we replicate PolyGCL, which frames self-supervised graph contrastive learning as a spectral-polynomial fusion of high- and low-pass graph filters. Our study mostly reproduced the reported gains on both homophilic and heterophilic graphs—confirming its ability to learn expressive node embeddings—while noting that results still depend on careful hyper-parameter calibration relying on labeled data, compromising the self-supervised approach.
              </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px 20px 49px 20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/pokemon_evolution.gif' style="width: 175px; height: 175px; object-fit: cover; object-position: 50% 5%;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/pokegans.pdf">
                <span class="papertitle">Pokégans: Using Generative Adversarial Networks to Create the Next Generation of Pokémons</span>
              </a>
              <br>
              Nicolai Hermann,
              <a href="https://www.linkedin.com/in/michael-h%C3%BCppe-1053b61a1">Michael Hüppe</a>
              <br>
              <a href="data/pokegans.pdf">Project Report</a>
              /
              <a href="https://github.com/nihermann/Pokemaenner">Code</a>
              <p></p>
              <p>
                In this paper, we present a compact pipeline that trains a bespoke generative adversarial network to create convincing “Fakémon” sprites. Using a curated, uniformly pre-processed corpus of original Pokémon graphics, the model learns franchise-specific aesthetics and outputs novel, game-ready creatures. The paper details data acquisition and network design, showcases fan-project applications, and highlights future optimisation paths to boost fidelity and training efficiency.
              </p>
            </td>
          </tr>


          <tr onmouseout="mc_stop()" onmouseover="mc_start()">
            <td style="padding:20px 20px 45px 20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mc_image'>
                  <video id="mc_video"
                         src="images/mc_localization.mov"
                         width="175" height="175"
                         muted playsinline preload="metadata"
                         style="object-fit:cover;object-position:50% 5%;position:absolute;top:0;left:0;opacity:0;transition:opacity .2s">
                    <!-- Fallback text -->
                    Your browser doesn’t support embedded video.
                  </video>
                </div>
                <img src='images/mc_localization.png' style="width: 175px; height: 175px; object-fit: cover; object-position: 50% 5%;">
              </div>
              <script type="text/javascript">
                const vid = document.getElementById('mc_video');

                function mc_start () {
                  // show + start from the beginning
                  vid.currentTime = 0;
                  vid.play();
                  vid.style.opacity = '1';
                }

                function mc_stop () {
                  // hide + pause to save CPU
                  vid.pause();
                  vid.currentTime = 0;      // reset so it restarts next hover
                  vid.style.opacity = '0';
                }
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/Markov_Localization___Final_Robotics_Project.pdf">
                <span class="papertitle">Robotics: Markov Localization and Monte Carlo Localization</span>
              </a>
              <br>
              Nicolai Hermann,
              <a href="https://www.linkedin.com/in/michele-cattaneo-614527210/">Michele Cattaneo</a>,
              <a href="https://www.linkedin.com/in/oliver-tryding-954171266/">Oliver Tryding</a>
              <br>
              <a href="data/Markov_Localization___Final_Robotics_Project.pdf">Project Report</a>
              /
              <a href="https://github.com/MicheleCattaneo/Markov_and_Monte_Carlo_Localization">Code</a>
              <p></p>
              <p>
                In this project, we built a small simulator and implemented Markov localization to maintain a full probability grid of robot poses after each motion and sensor reading. For larger or continuous maps, we switched to Monte Carlo localization, replacing the exhaustive grid with a compact set of weighted particles (see blue points in the animation).
              </p>
            </td>
          </tr>


          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
