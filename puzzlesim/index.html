
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Puzzle Similarity</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://nihermann.github.io/images/puzzle_after.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1297">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://nihermann.github.io/puzzlesim/index.html"/>
    <meta property="og:title" content="Puzzle Similiarty: A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions" />
    <meta property="og:description" content="Modern reconstruction techniques can effectively model complex 3D scenes from sparse 2D views. However, automatically assessing the quality of novel views and identifying artifacts is challenging due to the lack of ground truth images and the limitations of no-reference image metrics in predicting detailed artifact maps. The absence of such quality metrics hinders accurate predictions of the quality of generated views and limits the adoption of post-processing techniques, such as inpainting, to enhance reconstruction quality. In this work, we propose a new no-reference metric, Puzzle Similarity, which is designed to localize artifacts in novel views. Our approach utilizes image patch statistics from the input views to establish a scene-specific distribution that is later used to identify poorly reconstructed regions in the novel views. We test and evaluate our method in the context of 3D reconstruction; to this end, we collected a novel dataset of human quality assessment in unseen reconstructed views. Through this dataset, we demonstrate that our method can not only successfully localize artifacts in novel views, correlating with human assessment, but do so without direct references. Surprisingly, our metric outperforms both no-reference metrics and popular full-reference image metrics. We can leverage our new metric to enhance applications like automatic image restoration, guided acquisition, or 3D reconstruction from sparse inputs." />


<link rel="icon" href="img/puzzle_piece.svg">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Puzzle Similarity</b>: A Perceptually-guided No-Reference Metric<br>for Artifact Detection in 3D Scene Reconstructions</br>
                <small>
                ArXiv 2024
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="../index.html">
                          Nicolai Hermann
                        </a>
                    </li>
                    <li>
                        <a href="https://arcanous98.github.io/">
                            Jorge Condor
                        </a>
                    </li>
                    <li>
                        <a href="https://www.pdf.inf.usi.ch/people/piotr/">
                          Piotr Didyk
                        </a>
                    </li>
                    <br>IDSIA-USI
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="data/Puzzle_Similarity_paper.pdf">
                            <image src="img/puzzle_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">-->
<!--                            <image src="img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                        <li>
                            <a href="https://github.com/nihermann/PuzzleSim">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Data</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="data/Puzzle_Similarity_Supplemental.pdf">
                                <image src="img/puzzle_supplemental_image.jpg" height="60px">
                                    <h4><strong>Supplemental</strong></h4>
                            </a>
                        </li>

<!--                        <li>-->
<!--                            <a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_camp_zipnerf.ipynb">-->
<!--                            <image src="img/VertexAI-512-color.webp" height="60px">-->
<!--                                <h4><strong>Google Vertex AI Notebook</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <video id="v0" width="100%" autoplay loop muted controls>-->
<!--                  <source src="img/teaser.mp4" type="video/mp4" />-->
<!--                </video>-->
<!--						</div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Modern reconstruction techniques can effectively model complex 3D scenes from sparse 2D views. However, automatically assessing the quality of novel views and identifying artifacts is challenging due to the lack of ground truth images and the limitations of No-Reference image metrics in predicting reliable artifact maps. The absence of such metrics hinders assessment of the quality of novel views and limits the adoption of post-processing techniques, such as inpainting, to enhance reconstruction quality. To tackle this, recent work has established a new category of metrics (Cross-Reference), predicting image quality solely by leveraging context from alternate viewpoint captures. In this work, we propose a new Cross-Reference metric, Puzzle Similarity, which is designed to localize artifacts in novel views. Our approach utilizes image patch statistics from the input views to establish a scene-specific distribution, later used to identify poorly reconstructed regions in the novel views. Given the lack of good measures to evaluate CrossReference methods in the context of 3D reconstruction, we collected a novel human-labeled dataset of artifact and distortion maps in unseen reconstructed views. Through this dataset, we demonstrate that our method achieves state of the art localization of artifacts in novel views, correlating with human assessment, even without aligned references. We can leverage our new metric to enhance applications like automatic image restoration, guided acquisition, or 3D reconstruction from sparse inputs.
                </p>
            </div>
        </div>


<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Video-->
<!--                </h3>-->
<!--                <div class="text-center">-->
<!--                    <div style="position:relative;padding-top:56.25%;">-->
<!--                        <iframe src="https://youtube.com/embed/xrrhynRzC8k" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->
<!--<br>-->
<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    360° Video Flythroughs-->
<!--                </h3>-->
<!--                <div class="text-center">-->
<!--                    <div style="position:relative;padding-top:56.25%;">-->
<!--                        <iframe src="https://www.youtube.com/embed/videoseries?list=PLzPoYEE6Aw7Jzjek1uEIPnpcTDL3u8tb4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--                        &lt;!&ndash; <iframe src="https://youtube.com/embed/jbE2ri8xEZo" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> &ndash;&gt;-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->
<!--<br>-->
<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Multisampling-->
<!--                </h3>-->
<!--				<table style="width: 100%; border-collapse: collapse;">-->
<!--				  <tr>-->
<!--				    <td style="text-align: center;">-->
<!--		                <video id="v0" width="100%" autoplay loop muted>-->
<!--		                  <source src="img/hexify_train.mp4" type="video/mp4" />-->
<!--		                </video>-->
<!--					</td>-->
<!--				    <td style="text-align: center;">-->
<!--		                <video id="v0" width="100%" autoplay loop muted>-->
<!--		                  <source src="img/hexify_test.mp4" type="video/mp4" />-->
<!--		                </video>-->
<!--					</td>-->
<!--				  </tr>-->
<!--				  <tr>-->
<!--				    <td style="text-align: center;">When Training</td>-->
<!--				    <td style="text-align: center;">When Rendering</td>-->
<!--				  </tr>-->
<!--				</table>-->
<!--                <p class="text-justify">-->
<!--                    We use multisampling to approximate the average NGP feature over a conical frustum, by constructing a 6-sample pattern that exactly matches the frustum's first and second moments. When training, we randomly rotate and flip (along the ray axis) each pattern, and when rendering we deterministically flip and rotate each adjacent pattern by 30 degrees.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->
<!--<br>-->
<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    XY aliasing-->
<!--                </h3>-->
<!--                <video class="video" width=100% id="xyalias" loop playsinline autoplay muted src="img/xy_alias_swipe_crf27.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                <canvas height=0 class="videoMerge" id="xyaliasMerge"></canvas>-->
<!--                <p class="text-justify">-->
<!--                    A naive baseline (left) combining mip-NeRF 360 and Instant NGP results in aliasing as the camera moves laterally. Our full method (right) produces prefiltered renderings that do not flicker or shimmer.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->
<!--        <br>-->
<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    XY aliasing-->
<!--                </h3>-->
<!--                <video class="video" width=100% id="xyalias" loop playsinline autoplay muted src="img/xy_alias_swipe_crf27.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                <canvas height=0 class="videoMerge" id="xyaliasMerge"></canvas>-->
<!--                <p class="text-justify">-->
<!--                    A naive baseline (left) combining mip-NeRF 360 and Instant NGP results in aliasing as the camera moves laterally. Our full method (right) produces prefiltered renderings that do not flicker or shimmer.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Z aliasing-->
<!--                </h3>-->
<!--                <video id="v0" width="100%" autoplay loop muted>-->
<!--                  <source src="img/z_alias_pdf_labeled.m4v" type="video/mp4" />-->
<!--                </video>-->
<!--                <p class="text-justify">-->
<!--                    The proposal network used for resampling points along rays in mip-NeRF 360 results in an artifact we refer to as <em>z-aliasing</em>, where foreground content alternately appears and disappears as the camera moves toward or away from scene content. Z-aliasing occurs when the initial set of samples from the proposal network is not dense enough and misses thin structures, such as the chair above. Missed content can not be recovered by later rounds of sampling, since no future samples will be placed at that location along the ray. Our improvements to proposal network supervision result in a prefiltered proposal output that preserves the foreground object for all frames in this sequence. The plots above depict samples along a ray for three rounds of resampling (blue, orange, and green lines), with the y axis showing rendering weight (how much each interval contributes to the final rendered color), as a normalized probability density.-->
<!--                </p>-->
<!--            </div>-->
<!--        </div>-->

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Application
                    </h3>
                    <image src="img/applications.svg" width="100%">
                    <image src="img/applications-2.svg" width="100%">
                    <p class="text-justify">
                        <br>
                        Our metric can also be applied in automatic restoration of novel images from a reconstructed scene. Whenever it is possible to establish a visual distribution (e.g. we have a training dataset available), we can recursively use our metric to automatically identify visual outliers in novel views and remove them through neural inpainting.
                    </p>
                </div>
            </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@misc{hermann2024puzzlesim,
      title={Puzzle Similarity: A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions},
      author={Nicolai Hermann and Jorge Condor and Piotr Didyk},
      year={2024},
      eprint={2411.17489},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.17489},
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    We would like to thank Krzysztof Wolski for making their image segmentation tool available to us, Volodymyr Kyrylov for providing the idea and first prototype of the memory-efficient implementation, and Sophie Kergaßner for designing figures. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement N° 804226 PERDY), from the Swiss National Science Foundation (SNSF, Grant 200502) and an academic gift from Meta.
                    <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
